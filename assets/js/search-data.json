{
  
    
        "post0": {
            "title": "Car detection with RetinaNet",
            "content": "Setting up Fastai . Importing fastai and downgrading to lower PyTorch version. . from fastai import * from fastai.vision import * from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs !pip install &quot;torch==1.4&quot; &quot;torchvision==0.5.0&quot; . Requirement already satisfied: torch==1.4 in /usr/local/lib/python3.7/dist-packages (1.4.0) Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.7/dist-packages (0.5.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.19.5) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.15.0) Requirement already satisfied: pillow&gt;=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (7.1.2) . Building Upsampling function . class LateralUpsampleMerge(nn.Module): &quot;Merge the features coming from the downsample path (in `hook`) with the upsample path.&quot; def __init__(self, ch, ch_lat, hook): super().__init__() self.hook = hook self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True) def forward(self, x): return self.conv_lat(self.hook.stored) + F.interpolate(x, self.hook.stored.shape[-2:], mode=&#39;nearest&#39;) . Building RetinaNet . RetinaNet is an object detection model building upon ResNet and uses a Feature Pyramid Network to generate a convolutional feature pyramid in which there is attached two subnetworks. . One of these networks is for classifying anchor boxes and the other is for regressing anchor boxes to ground-truth objects. . class RetinaNet(nn.Module): &quot;Implements RetinaNet from https://arxiv.org/abs/1708.02002&quot; def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True): super().__init__() self.n_classes,self.flatten = n_classes,flatten imsize = (256,256) sfs_szs = model_sizes(encoder, size=imsize) sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs))) self.sfs = hook_outputs([encoder[i] for i in sfs_idxs]) self.encoder = encoder self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True) self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True) self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True)) self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) for idx,hook in zip(sfs_idxs[-2:-4:-1], self.sfs[-2:-4:-1])]) self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)]) self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs) self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs) self.res=conv2d(chs,chs,3,stride=1) def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256): &quot;Helper function to create one of the subnet for regression/classification.&quot; #layers = [ResBlock(chs) for _ in range(n_conv)] layers = [conv_layer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)] layers += [conv2d(chs, n_classes * n_anchors, bias=True)] layers[-1].bias.data.zero_().add_(final_bias) layers[-1].weight.data.fill_(0) return nn.Sequential(*layers) def _apply_transpose(self, func, p_states, n_classes): #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate #all the results in bs * anchors * k (the non flatten version is there for debugging only) if not self.flatten: sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states] return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)] else: return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1) def forward(self, x): c5 = self.encoder(x) p_states = [self.c5top5(c5.clone()), self.c5top6(c5)] p_states.append(self.p6top7(p_states[-1])) for merge in self.merges: p_states = [merge(p_states[0])] + p_states for i, smooth in enumerate(self.smoothers[:3]): p_states[i] = smooth(p_states[i]) return [self._apply_transpose(self.classifier, p_states, self.n_classes), self._apply_transpose(self.box_regressor, p_states, 4), [[p.size(2), p.size(3)] for p in p_states]] def __del__(self): if hasattr(self, &quot;sfs&quot;): self.sfs.remove() . Create anchors . Anchors are points on a grid that we will use to determine where the bounding boxes has to be. . def create_grid(size): &quot;Create a grid of a given `size`.&quot; H, W = size if is_tuple(size) else (size,size) grid = FloatTensor(H, W, 2) linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W &gt; 1 else tensor([0.]) grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0]) linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H &gt; 1 else tensor([0.]) grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1]) return grid.view(-1,2) def show_anchors(ancs, size): _,ax = plt.subplots(1,1, figsize=(5,5)) ax.set_xticks(np.linspace(-1,1, size[1]+1)) ax.set_yticks(np.linspace(-1,1, size[0]+1)) ax.grid() ax.scatter(ancs[:,1], ancs[:,0]) #y is first ax.set_yticklabels([]) ax.set_xticklabels([]) ax.set_xlim(-1,1) ax.set_ylim(1,-1) #-1 is top, 1 is bottom for i, (x, y) in enumerate(zip(ancs[:, 1], ancs[:, 0])): ax.annotate(i, xy = (x,y)) def create_anchors(sizes, ratios, scales, flatten=True): &quot;Create anchor of `sizes`, `ratios` and `scales`.&quot; aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios] aspects = torch.tensor(aspects).view(-1,2) anchors = [] for h,w in sizes: #4 here to have the anchors overlap. sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0) base_grid = create_grid((h,w)).unsqueeze(1) n,a = base_grid.size(0),aspects.size(0) ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2) anchors.append(ancs.view(h,w,a,4)) return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors . Draw on image . Functionality for drawing squares on the images. . import matplotlib.cm as cmx import matplotlib.colors as mcolors from cycler import cycler def get_cmap(N): color_norm = mcolors.Normalize(vmin=0, vmax=N-1) return cmx.ScalarMappable(norm=color_norm, cmap=&#39;Set3&#39;).to_rgba num_color = 12 cmap = get_cmap(num_color) color_list = [cmap(float(x)) for x in range(num_color)] def draw_outline(o, lw): o.set_path_effects([patheffects.Stroke( linewidth=lw, foreground=&#39;black&#39;), patheffects.Normal()]) def draw_rect(ax, b, color=&#39;white&#39;): patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2)) draw_outline(patch, 4) def draw_text(ax, xy, txt, sz=14, color=&#39;white&#39;): text = ax.text(*xy, txt, verticalalignment=&#39;top&#39;, color=color, fontsize=sz, weight=&#39;bold&#39;) . Display and compute anchors . Calculate where the bounding boxes has to be drawn as well as their IoU values (how much they overlap each other). . def show_boxes(boxes): &quot;Show the `boxes` (size by 4)&quot; _, ax = plt.subplots(1,1, figsize=(5,5)) ax.set_xlim(-1,1) ax.set_ylim(1,-1) for i, bbox in enumerate(boxes): bb = bbox.numpy() rect = [bb[1]-bb[3]/2, bb[0]-bb[2]/2, bb[3], bb[2]] draw_rect(ax, rect, color=color_list[i%num_color]) draw_text(ax, [bb[1]-bb[3]/2,bb[0]-bb[2]/2], str(i), color=color_list[i%num_color]) def activ_to_bbox(acts, anchors, flatten=True): &quot;Extrapolate bounding boxes on anchors from the model activations.&quot; if flatten: acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2] sizes = anchors[...,2:] * torch.exp(acts[...,:2]) return torch.cat([centers, sizes], -1) else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)] return res def cthw2tlbr(boxes): &quot;Convert center/size format `boxes` to top/left bottom/right corners.&quot; top_left = boxes[:,:2] - boxes[:,2:]/2 bot_right = boxes[:,:2] + boxes[:,2:]/2 return torch.cat([top_left, bot_right], 1) def intersection(anchors, targets): &quot;Compute the sizes of the intersections of `anchors` by `targets`.&quot; ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets) a, t = ancs.size(0), tgts.size(0) ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4) top_left_i = torch.max(ancs[...,:2], tgts[...,:2]) bot_right_i = torch.min(ancs[...,2:], tgts[...,2:]) sizes = torch.clamp(bot_right_i - top_left_i, min=0) return sizes[...,0] * sizes[...,1] def IoU_values(anchors, targets): &quot;Compute the IoU values of `anchors` by `targets`.&quot; inter = intersection(anchors, targets) anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3] union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter return inter/(union+1e-8) def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4): &quot;Match `anchors` to targets. -1 is match to background, -2 is ignore.&quot; matches = anchors.new(anchors.size(0)).zero_().long() - 2 if targets.numel() == 0: return matches ious = IoU_values(anchors, targets) vals,idxs = torch.max(ious,1) matches[vals &lt; bkg_thr] = -1 matches[vals &gt; match_thr] = idxs[vals &gt; match_thr] #Overwrite matches with each target getting the anchor that has the max IoU. #vals,idxs = torch.max(ious,0) #If idxs contains repetition, this doesn&#39;t bug and only the last is considered. #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long() return matches def tlbr2cthw(boxes): &quot;Convert top/left bottom/right format `boxes` to center/size corners.&quot; center = (boxes[:,:2] + boxes[:,2:])/2 sizes = boxes[:,2:] - boxes[:,:2] return torch.cat([center, sizes], 1) def bbox_to_activ(bboxes, anchors, flatten=True): &quot;Return the target of the model on `anchors` for the `bboxes`.&quot; if flatten: t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]])) else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)] return res def encode_class(idxs, n_classes): target = idxs.new_zeros(len(idxs), n_classes).float() mask = idxs != 0 i1s = LongTensor(list(range(len(idxs)))) target[i1s[mask],idxs[mask]-1] = 1 return target . Create RetinaNet loss function . class RetinaNetFocalLoss(nn.Module): def __init__(self, gamma:float=2., alpha:float=0.25, pad_idx:int=0, scales:Collection[float]=None, ratios:Collection[float]=None, reg_loss:LossFunction=F.smooth_l1_loss): super().__init__() self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)]) self.ratios = ifnone(ratios, [1/2,1,2]) def _change_anchors(self, sizes:Sizes) -&gt; bool: if not hasattr(self, &#39;sizes&#39;): return True for sz1, sz2 in zip(self.sizes, sizes): if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True return False def _create_anchors(self, sizes:Sizes, device:torch.device): self.sizes = sizes self.anchors = create_anchors(sizes, self.ratios, self.scales).to(device) # def _unpad(self, bbox_tgt, clas_tgt): # i = torch.min(torch.nonzero(clas_tgt-self.pad_idx)) # return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx def _unpad(self, bbox_tgt, clas_tgt): i = torch.min(torch.nonzero(clas_tgt - self.pad_idx)) if sum(clas_tgt)&gt;0 else 0 return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:] - 1 + self.pad_idx def _focal_loss(self, clas_pred, clas_tgt): encoded_tgt = encode_class(clas_tgt, clas_pred.size(1)) ps = torch.sigmoid(clas_pred.detach()) weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha) weights.pow_(self.gamma).mul_(alphas) clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction=&#39;sum&#39;) return clas_loss def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt): bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt) matches = match_anchors(self.anchors, bbox_tgt) bbox_mask = matches&gt;=0 if bbox_mask.sum() != 0: bbox_pred = bbox_pred[bbox_mask] bbox_tgt = bbox_tgt[matches[bbox_mask]] bb_loss = self.reg_loss(bbox_pred, bbox_to_activ(bbox_tgt, self.anchors[bbox_mask])) else: bb_loss = 0. matches.add_(1) clas_tgt = clas_tgt + 1 clas_mask = matches&gt;=0 clas_pred = clas_pred[clas_mask] clas_tgt = torch.cat([clas_tgt.new_zeros(1).long(), clas_tgt]) clas_tgt = clas_tgt[matches[clas_mask]] return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.) def forward(self, output, bbox_tgts, clas_tgts): clas_preds, bbox_preds, sizes = output if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device) n_classes = clas_preds.size(2) return sum([self._one_loss(cp, bp, ct, bt) for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0) . ratios = [1/2,1,2] scales = [1,2**(-1/3), 2**(-2/3)] # scales = [1,2**(1/3), 2**(2/3)] #for bigger size . def get_predictions(output, idx, detect_thresh=0.05): bbox_pred, scores, preds = process_output(output, idx, detect_thresh) if len(scores) == 0: return [],[],[] to_keep = nms(bbox_pred, scores) return bbox_pred[to_keep], preds[to_keep], scores[to_keep] def compute_ap(precision, recall): &quot;Compute the average precision for `precision` and `recall` curve.&quot; recall = np.concatenate(([0.], list(recall), [1.])) precision = np.concatenate(([0.], list(precision), [0.])) for i in range(len(precision) - 1, 0, -1): precision[i - 1] = np.maximum(precision[i - 1], precision[i]) idx = np.where(recall[1:] != recall[:-1])[0] ap = np.sum((recall[idx + 1] - recall[idx]) * precision[idx + 1]) return ap def compute_class_AP(model, dl, n_classes, iou_thresh=0.5, detect_thresh=0.35, num_keep=100): tps, clas, p_scores = [], [], [] classes, n_gts = LongTensor(range(n_classes)),torch.zeros(n_classes).long() with torch.no_grad(): for input,target in progress_bar(dl): output = model(input) for i in range(target[0].size(0)): bbox_pred, preds, scores = get_predictions(output, i, detect_thresh) tgt_bbox, tgt_clas = unpad(target[0][i], target[1][i]) if len(bbox_pred) != 0 and len(tgt_bbox) != 0: ious = IoU_values(bbox_pred, tgt_bbox) max_iou, matches = ious.max(1) detected = [] for i in range_of(preds): if max_iou[i] &gt;= iou_thresh and matches[i] not in detected and tgt_clas[matches[i]] == preds[i]: detected.append(matches[i]) tps.append(1) else: tps.append(0) clas.append(preds.cpu()) p_scores.append(scores.cpu()) n_gts += (tgt_clas.cpu()[:,None] == classes[None,:]).sum(0) tps, p_scores, clas = torch.tensor(tps), torch.cat(p_scores,0), torch.cat(clas,0) fps = 1-tps idx = p_scores.argsort(descending=True) tps, fps, clas = tps[idx], fps[idx], clas[idx] aps = [] #return tps, clas for cls in range(n_classes): tps_cls, fps_cls = tps[clas==cls].float().cumsum(0), fps[clas==cls].float().cumsum(0) if tps_cls.numel() != 0 and tps_cls[-1] != 0: precision = tps_cls / (tps_cls + fps_cls + 1e-8) recall = tps_cls / (n_gts[cls] + 1e-8) aps.append(compute_ap(precision, recall)) else: aps.append(0.) return aps . def _unpad(self, bbox_tgt, clas_tgt): i = torch.min(torch.nonzero(clas_tgt - self.pad_idx)) if sum(clas_tgt)&gt;0 else 0 return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:] - 1 + self.pad_idx def _draw_outline(o:Patch, lw:int): &quot;Outline bounding box onto image `Patch`.&quot; o.set_path_effects([patheffects.Stroke( linewidth=lw, foreground=&#39;black&#39;), patheffects.Normal()]) def draw_rect(ax:plt.Axes, b:Collection[int], color:str=&#39;white&#39;, text=None, text_size=14): &quot;Draw bounding box on `ax`.&quot; patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2)) _draw_outline(patch, 4) if text is not None: patch = ax.text(*b[:2], text, verticalalignment=&#39;top&#39;, color=color, fontsize=text_size, weight=&#39;bold&#39;) _draw_outline(patch,1) def nms(boxes, scores, thresh=0.3): idx_sort = scores.argsort(descending=True) boxes, scores = boxes[idx_sort], scores[idx_sort] to_keep, indexes = [], torch.LongTensor(range_of(scores)) while len(scores) &gt; 0: to_keep.append(idx_sort[indexes[0]]) iou_vals = IoU_values(boxes, boxes[:1]).squeeze() mask_keep = iou_vals &lt; thresh if len(mask_keep.nonzero()) == 0: break boxes, scores, indexes = boxes[mask_keep], scores[mask_keep], indexes[mask_keep] return LongTensor(to_keep) def process_output(output, i, detect_thresh=0.25): clas_pred,bbox_pred,sizes = output[0][i], output[1][i], output[2] anchors = create_anchors(sizes, ratios, scales).to(clas_pred.device) bbox_pred = activ_to_bbox(bbox_pred, anchors) clas_pred = torch.sigmoid(clas_pred) detect_mask = clas_pred.max(1)[0] &gt; detect_thresh bbox_pred, clas_pred = bbox_pred[detect_mask], clas_pred[detect_mask] bbox_pred = tlbr2cthw(torch.clamp(cthw2tlbr(bbox_pred), min=-1, max=1)) if clas_pred.numel() == 0: return [],[],[] scores, preds = clas_pred.max(1) return bbox_pred, scores, preds def show_preds(img, output, idx, detect_thresh=0.35, classes=None, ax=None): bbox_pred, scores, preds = process_output(output, idx, detect_thresh) if len(scores) != 0: to_keep = nms(bbox_pred, scores) bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu() t_sz = torch.Tensor([*img.size])[None].float() bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2 bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2 bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz bbox_pred = bbox_pred.long() if ax is None: fig, ax = plt.subplots(1,1) axs=img.show(ax=ax) for bbox, c, scr in zip(bbox_pred, preds, scores): txt = str(c.item()) if classes is None else classes[c.item()+1] draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f&#39;{txt} {scr:.2f}&#39;) def show_results(learn, start=0, n=3, detect_thresh=0.35, figsize=(10,25)): x,y = next(iter(learn.data.valid_dl)) #one_batch(DatasetType.Valid, cpu=False) with torch.no_grad(): z = learn.model.eval()(x) _,axs = plt.subplots(n, 1, figsize=figsize) for i in range(n): img,bbox = learn.data.valid_ds[start+i] #img.show(ax=axs[i,0], y=bbox) show_preds(img, z, start+i, detect_thresh=detect_thresh, classes=learn.data.classes, ax=axs[i]) . Data manipulation . Mounting Google Drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . Dataset . Unzipping . !unzip -u drive/MyDrive/&#39;Colab Notebooks/ZIP files/Self Driving Car.v3-fixed-small.coco.zip&#39; -d drive/MyDrive/&#39;Colab Notebooks/Data/Cars/&#39; . Moving json from training folder . !mv &#39;drive/MyDrive/Colab Notebooks/Data/Cars/export/_annotations.coco.json&#39; &#39;drive/MyDrive/Colab Notebooks/Data/Cars/&#39; . mv: cannot stat &#39;drive/MyDrive/Colab Notebooks/Data/Cars/export/_annotations.coco.json&#39;: No such file or directory . ls -l &#39;drive/MyDrive/Colab Notebooks/Data/Cars/&#39;*.json . -rw- 1 root root 37640268 Mar 10 2020 &#39;drive/MyDrive/Colab Notebooks/Data/Cars/_annotations.coco.json&#39; . Setting path to image folder . path=Path(&#39;drive/MyDrive/Colab Notebooks/Data/Cars/&#39;) . Loading image annotations . import json trn_j = json.load((path/&#39;_annotations.coco.json&#39;).open()) trn_j.keys() . dict_keys([&#39;info&#39;, &#39;licenses&#39;, &#39;categories&#39;, &#39;images&#39;, &#39;annotations&#39;]) . images,annotations,categories = [&#39;images&#39;,&#39;annotations&#39;,&#39;categories&#39;] . trn_j[images][0] . {&#39;date_captured&#39;: &#39;2020-03-10T12:15:29+00:00&#39;, &#39;file_name&#39;: &#39;1478898964443478170_jpg.rf.0032b18b04e80a46564f82da1c1a710c.jpg&#39;, &#39;height&#39;: 512, &#39;id&#39;: 0, &#39;license&#39;: 1, &#39;width&#39;: 512} . len(trn_j[images]) . trn_j[annotations][0] . {&#39;area&#39;: 3454.2933333333335, &#39;bbox&#39;: [107, 255, 58.666666666666664, 58.88], &#39;category_id&#39;: 2, &#39;id&#39;: 0, &#39;image_id&#39;: 0, &#39;iscrowd&#39;: 0, &#39;segmentation&#39;: []} . len(trn_j[annotations]) . trn_j[categories] . [{&#39;id&#39;: 0, &#39;name&#39;: &#39;obstacles&#39;, &#39;supercategory&#39;: &#39;none&#39;}, {&#39;id&#39;: 1, &#39;name&#39;: &#39;biker&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 2, &#39;name&#39;: &#39;car&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 3, &#39;name&#39;: &#39;pedestrian&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 4, &#39;name&#39;: &#39;trafficLight&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 5, &#39;name&#39;: &#39;trafficLight-Green&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 6, &#39;name&#39;: &#39;trafficLight-GreenLeft&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 7, &#39;name&#39;: &#39;trafficLight-Red&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 8, &#39;name&#39;: &#39;trafficLight-RedLeft&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 9, &#39;name&#39;: &#39;trafficLight-Yellow&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 10, &#39;name&#39;: &#39;trafficLight-YellowLeft&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}, {&#39;id&#39;: 11, &#39;name&#39;: &#39;truck&#39;, &#39;supercategory&#39;: &#39;obstacles&#39;}] . len(trn_j[categories]) . Filtering data . Create list of file names which has annotations. . file_name,id,img_id,cat_id,bbox = &#39;file_name&#39;,&#39;id&#39;,&#39;image_id&#39;,&#39;category_id&#39;,&#39;bbox&#39; trn_img_ids = [o[img_id] for o in trn_j[annotations]] unique_ids = set(trn_img_ids) trn_fns_unique_ids = [i for i in unique_ids] trn_fns_id = [o[id] for o in trn_j[images]] trn_fns_fn = [o[file_name] for o in trn_j[images]] trn_fns_fn_id = list(zip(trn_fns_id, trn_fns_fn)) trn_fns = [i[1] for i in trn_fns_fn_id if i[0] in trn_fns_unique_ids] . Delete unwanted images. . trn_fns_delete_files = [i for i in trn_fns_fn if i not in trn_fns] . for i in trn_fns_delete_files: try: os.remove(&#39;/content/drive/MyDrive/Colab Notebooks/Data/Cars/export/&#39; + i) except: print(&#39;Error at &#39; + i) . Zipping the list of annotated images with their respected bounding box values . images_trn,lbl_bbox_trn=get_annotations(path/&#39;_annotations.coco.json&#39;) lbl_bbox=lbl_bbox_trn img2bbox = dict(zip(trn_fns, lbl_bbox)) get_y_func = lambda o:img2bbox[o.name] . img2bbox . def get_data(bs, size): src = ObjectItemList.from_folder(path/&#39;export&#39;) src = src.split_by_rand_pct(0.4,seed=2) src = src.label_from_func(get_y_func) src = src.transform(get_transforms(), size=size, tfm_y=True) return src.databunch(path=path, bs=bs, collate_fn=bb_pad_collate) . Display batch sample . data = get_data(64,128) data.show_batch(rows=3) . /usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return np.array(a, dtype=dtype, **kwargs) . Create learner based on RetinaNet . encoder = create_body(models.resnet50, cut=-2) model = RetinaNet(encoder, data.c, final_bias=-4) crit = RetinaNetFocalLoss(scales=scales, ratios=ratios) learn = Learner(data, model, loss_func=crit) . Downloading: &#34;https://download.pytorch.org/models/resnet50-19c8e357.pth&#34; to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth . . def retina_net_split(model): groups = [list(model.encoder.children())[:6], list(model.encoder.children())[6:]] return groups + [list(model.children())[1:]] . learn = learn.split(retina_net_split) . Training . Debug Out of Memory . !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi !pip install gputil !pip install psutil !pip install humanize import psutil import humanize import os import GPUtil as GPU import gc . Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0) Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8) Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1) . gc.collect() torch.cuda.empty_cache() GPUs = GPU.getGPUs() # XXX: only one GPU on Colab and isn’t guaranteed gpu = GPUs[0] def printm(): process = psutil.Process(os.getpid()) print(&quot;Gen RAM Free: &quot; + humanize.naturalsize(psutil.virtual_memory().available), &quot; | Proc size: &quot; + humanize.naturalsize(process.memory_info().rss)) print(&quot;GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB&quot;.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal)) printm() . Gen RAM Free: 11.6 GB | Proc size: 2.0 GB GPU RAM Free: 10982MB | Used: 459MB | Util 4% | Total 11441MB . Training and fine tuning . learn.freeze() learn.save(&#39;stage-0&#39;) . learn.lr_find() . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss time . . 46.77% [58/124 01:52&lt;02:07 8.9884] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.recorder.plot() . learn = learn.load(&#39;stage-0&#39;) . learn.fit_one_cycle(10,max_lr=(4e-4), pct_start=0.8) . epoch train_loss valid_loss time . 0 | 2.372360 | 2.258969 | 13:43 | . 1 | 1.978313 | 1.655697 | 05:13 | . 2 | 1.532820 | 1.320511 | 05:14 | . 3 | 1.289616 | 1.288068 | 05:15 | . 4 | 1.119344 | 0.981065 | 05:15 | . 5 | 1.103997 | 1.035723 | 05:16 | . 6 | 0.995536 | 0.848817 | 05:15 | . 7 | 0.943265 | 0.824874 | 05:15 | . 8 | 0.868626 | 0.764294 | 05:15 | . 9 | 0.776386 | 0.711596 | 05:16 | . learn.fit_one_cycle(5, max_lr=(1e-4)) . epoch train_loss valid_loss time . 0 | 0.752578 | 0.729032 | 05:17 | . 1 | 0.749534 | 0.703780 | 05:17 | . 2 | 0.725929 | 0.662285 | 05:17 | . 3 | 0.698704 | 0.671640 | 05:15 | . 4 | 0.694300 | 0.661299 | 05:15 | . learn.save(&#39;stage-1-128&#39;) learn=learn.load(&#39;stage-1-128&#39;) learn.unfreeze() . learn.fit_one_cycle(5,max_lr=slice(1e-6,5e-5),wd=0.1) . epoch train_loss valid_loss time . 0 | 0.678424 | 0.660980 | 05:29 | . 1 | 0.682555 | 0.637388 | 05:27 | . 2 | 0.669539 | 0.620335 | 05:27 | . 3 | 0.647839 | 0.631366 | 05:26 | . 4 | 0.643494 | 0.618361 | 05:26 | . learn.save(&#39;stage-2-128&#39;) . learn=learn.load(&#39;stage-2-128&#39;) . learn.data = get_data(32,192) . /usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return np.array(a, dtype=dtype, **kwargs) . learn.freeze() . learn.lr_find() . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss time . . 22.18% [55/248 01:24&lt;04:56 1.2236] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.recorder.plot() . learn.fit_one_cycle(10,5e-5) . epoch train_loss valid_loss time . 0 | 0.789636 | 0.747257 | 08:16 | . 1 | 0.754189 | 0.719629 | 08:15 | . 2 | 0.735641 | 0.695408 | 08:16 | . 3 | 0.712848 | 0.679594 | 08:15 | . 4 | 0.691196 | 0.669170 | 08:22 | . 5 | 0.672254 | 0.646864 | 08:19 | . 6 | 0.660691 | 0.635468 | 08:20 | . 7 | 0.646692 | 0.646163 | 08:22 | . 8 | 0.636073 | 0.635699 | 08:21 | . 9 | 0.645145 | 0.635715 | 08:20 | . learn.fit_one_cycle(5,1e-5) . . 40.00% [2/5 16:40&lt;25:01] epoch train_loss valid_loss time . 0 | 0.669853 | 0.652957 | 08:20 | . 1 | 0.676315 | 0.639512 | 08:20 | . . 71.37% [177/248 04:29&lt;01:47 0.6635] &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.save(&#39;stage-1-192&#39;) . learn=learn.load(&#39;stage-1-192&#39;) . learn.unfreeze() . learn.fit_one_cycle(2,max_lr=slice(1e-4,5e-4),wd=0.1) . learn.save(&#39;stage-2-192&#39;) . learn=learn.load(&#39;stage-2-192&#39;) . learn.data=get_data(24,256) . /usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return np.array(a, dtype=dtype, **kwargs) . learn.freeze() . learn.lr_find() . learn.recorder.plot() . learn.fit_one_cycle(5,1e-5) . learn.fit_one_cycle(3,1e-5) . learn.save(&#39;stage-1-256&#39;) . learn=learn.load(&#39;stage-1-256&#39;) . learn.unfreeze() . learn.lr_find() . . 0.00% [0/1 00:00&lt;00:00] epoch train_loss valid_loss time . . 16.92% [56/331 02:46&lt;13:35 1.7158] &lt;/div&gt; &lt;/div&gt; LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.recorder.plot() . learn.fit_one_cycle(10,max_lr=slice(1e-5,5e-5),wd=0.1) . epoch train_loss valid_loss time . 0 | 0.645950 | 0.611335 | 22:52 | . 1 | 0.656792 | 0.604118 | 07:13 | . 2 | 0.632400 | 0.600813 | 07:23 | . 3 | 0.619777 | 0.583280 | 07:23 | . 4 | 0.604613 | 0.610978 | 07:21 | . 5 | 0.569191 | 0.549718 | 07:17 | . 6 | 0.540223 | 0.547170 | 07:16 | . 7 | 0.527198 | 0.538132 | 07:16 | . 8 | 0.523622 | 0.535522 | 07:23 | . 9 | 0.510464 | 0.532288 | 07:22 | . learn.save(&#39;stage-2-256&#39;) . learn=learn.load(&#39;stage-2-256&#39;) . Evaluation . learn.model.eval() . RetinaNet( (encoder): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (5): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (6): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (7): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) (c5top5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1)) (c5top6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (p6top7): Sequential( (0): ReLU() (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) ) (merges): ModuleList( (0): LateralUpsampleMerge( (conv_lat): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)) ) (1): LateralUpsampleMerge( (conv_lat): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)) ) ) (smoothers): ModuleList( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (classifier): Sequential( (0): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (3): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (4): Conv2d(256, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (box_regressor): Sequential( (0): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (3): Sequential( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) ) (4): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) (res): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) ) . Show result with predictions . show_results(learn, start=5, n=15, detect_thresh=0.70, figsize=(50,125)) . learn.export(os.path.abspath(path/&#39;models/Car_detection.pkl&#39;)) . &lt;/div&gt; .",
            "url": "https://emil428f.github.io/Pba.Blog/2021/05/24/Car-Detection.html",
            "relUrl": "/2021/05/24/Car-Detection.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Create your own implementation of Learner from scratch, based on the training loop shown in this chapter. | Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You&#39;ll need to do some of your own research to figure out how to overcome some obstacles you&#39;ll meet on the way. | Imports . fastai rollercoaster . Arrange float between 0..20 to time using torch.arange() . time = torch.arange(0,20).float() . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2+1 . Define function for predictions using time and weight as parameters. . def f(t, weight): a,b,c = weight return a*(t**2) + (b*t) + c . Define the loss function (in this case we will use mse) using the prediction and target as parameters. . def mean_squared_error(prediction,targets): return ((prediction-targets)**2).mean().sqrt() . Step 1: Initialize the parameters . This is done by creating a variable to hold this information, in our case this will be &quot;weight&quot; singular. . We call the PyTorch function for providing random values torch.randm() and we want to track the gradient of these values. . weight = torch.randn(3).requires_grad_() . Step 2: Calculate the predictions . prediction = f(time, weight) . def show_prediction(prediction, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(prediction), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_prediction(prediction) . Step 3: Calculate the loss . Use MSE to calculate loss using prediction and rate. . loss = mean_squared_error(prediction, speed) loss . tensor(160.6979, grad_fn=&lt;SqrtBackward&gt;) . Step 4: Calculate the gradients . Use the backward function on the loss variable. Afterwards calculate gradient on the weight. . loss.backward() weight.grad . tensor([-165.5151, -10.6402, -0.7900]) . Step 5: Step the weights. . learning_rate = 0.00001 weight.data -= learning_rate * weight.grad.data weight.grad = None . prediction = f(time, weight) mean_squared_error(prediction, speed) . tensor(160.4228, grad_fn=&lt;SqrtBackward&gt;) . show_prediction(prediction) . def apply_step(weight, prn=True): prediction = f(time, weight) loss = mean_squared_error(weight, speed) loss.backward() weight.data -= learning_rate * weight.grad.data weight.data = None if prn: print(loss.item()) return prediction . Step 6: Repeat the process . for i in range(10): apply_step(weight) . RuntimeError Traceback (most recent call last) &lt;ipython-input-18-d95c0f21d2e7&gt; in &lt;module&gt; -&gt; 1 for i in range(10): apply_step(weight) &lt;ipython-input-17-68158542c5bb&gt; in apply_step(weight, prn) 1 def apply_step(weight, prn=True): 2 prediction = f(time, weight) -&gt; 3 loss = mean_squared_error(weight, speed) 4 loss.backward() 5 weight.data -= learning_rate * weight.grad.data &lt;ipython-input-6-11554d327303&gt; in mean_squared_error(prediction, targets) -&gt; 1 def mean_squared_error(prediction,targets): return ((prediction-targets)**2).mean().sqrt() RuntimeError: The size of tensor a (3) must match the size of tensor b (20) at non-singleton dimension 0 . Step 7: stop .",
            "url": "https://emil428f.github.io/Pba.Blog/2021/03/08/Rollercoaster-Learning-Algorithm.html",
            "relUrl": "/2021/03/08/Rollercoaster-Learning-Algorithm.html",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "From Model to Production",
            "content": "import PIL . The Practice of Deep Learning . Starting Your Project . The State of Deep Learning . Computer vision . Text (natural language processing) . Combining text and images . Tabular data . Recommendation systems . Other data types . The Drivetrain Approach . Gathering Data . clean . To download images with Bing Image Search, sign up at Microsoft Azure for a free account. You will be given a key, which you can copy and enter in a cell as follows (replacing &#39;XXX&#39; with your key and executing it): . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;1be3ada0f2a649d087e9fb39798485a0&#39;) . def search_images_bing_new(key, term, customConfigId, min_sz=128): url = &#39;https://api.bing.microsoft.com/v7.0/custom/images/search?&#39; + &#39;q=&#39; + term + &#39;&amp;&#39; + &#39;customconfig=&#39; + customConfigId + &#39;&amp;&#39; + &#39;count=150&#39; r = requests.get(url, headers={&#39;Ocp-Apim-Subscription-Key&#39;: key}) search_results = r.json() return L([img[&quot;thumbnailUrl&quot;] + &#39;.jpg&#39; for img in search_results[&quot;value&quot;][:150]]) . results = search_images_bing_new(key, &#39;hotdog&#39;, &#39;efb1b149-c219-4091-be6d-14a279c405c4&#39;) ims = results.attrgot(&#39;contentUrl&#39;) len(ims) . 150 . results . (#150) [&#39;https://tse2.mm.bing.net/th?id=OIP.WwNG0Db_o58XuR8uyHFGJQHaE8&amp;pid=Api.jpg&#39;,&#39;https://tse3.mm.bing.net/th?id=OIP.1Xkr3YbMeVXI-QJL8w6AigHaDt&amp;pid=Api.jpg&#39;,&#39;https://tse2.mm.bing.net/th?id=OIP.QHEOpw8npesuG0h6ojJ0HwHaFj&amp;pid=Api.jpg&#39;,&#39;https://tse3.mm.bing.net/th?id=OIP.UyKMCHBsnEpwRGlJmu1HWQHaE8&amp;pid=Api.jpg&#39;,&#39;https://tse2.mm.bing.net/th?id=OIP.3IVuG7q7Q3B_BmyXkvjz0gHaFj&amp;pid=Api.jpg&#39;,&#39;https://tse4.mm.bing.net/th?id=OIP.uovloOsMkI8LfzTheiLThQHaE6&amp;pid=Api.jpg&#39;,&#39;https://tse4.mm.bing.net/th?id=OIP.au202vJwBfwntBr4Byjw5wHaFj&amp;pid=Api.jpg&#39;,&#39;https://tse1.mm.bing.net/th?id=OIP.nE5Gj2cAgzraJHOhzBG65AHaEK&amp;pid=Api.jpg&#39;,&#39;https://tse3.mm.bing.net/th?id=OIP.WqTuusD_RbVuMEIbIDlFsQHaD4&amp;pid=Api.jpg&#39;,&#39;https://tse1.mm.bing.net/th?id=OIP.P8GRsSEg7Eh6btc5vTDvCAHaEK&amp;pid=Api.jpg&#39;...] . dest = &#39;images/hotdog.jpg&#39; download_url(results[0], dest) . FileNotFoundError Traceback (most recent call last) &lt;ipython-input-7-47f015778a8f&gt; in &lt;module&gt; 1 dest = &#39;images/hotdog.jpg&#39; -&gt; 2 download_url(results[0], dest) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/external.py in download_url(url, dest, overwrite, pbar, show_progress, chunk_size, timeout, retries) 166 except: show_progress = False 167 --&gt; 168 with open(dest, &#39;wb&#39;) as f: 169 nbytes = 0 170 if show_progress: pbar = progress_bar(range(file_size), leave=False, parent=pbar) FileNotFoundError: [Errno 2] No such file or directory: &#39;images/hotdog.jpg&#39; . im = Image.open(&#39;FastFood/Hotdog/00000019.jpg&#39;) im.to_thumb(128,128) . hotdog_type = &#39;Hotdog&#39;, &#39;Not Hotdog&#39; path = Path(&#39;FastFood&#39;) . if not path.exists(): path.mkdir() for o in hotdog_type: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing_new(key, o, &#39;efb1b149-c219-4091-be6d-14a279c405c4&#39;) download_images(dest, urls=results) . fns = get_image_files(path) fns . (#300) [Path(&#39;FastFood/Hotdog/00000003.jpg&#39;),Path(&#39;FastFood/Hotdog/00000000.jpg&#39;),Path(&#39;FastFood/Hotdog/00000006.jpg&#39;),Path(&#39;FastFood/Hotdog/00000005.jpg&#39;),Path(&#39;FastFood/Hotdog/00000007.jpg&#39;),Path(&#39;FastFood/Hotdog/00000001.jpg&#39;),Path(&#39;FastFood/Hotdog/00000004.jpg&#39;),Path(&#39;FastFood/Hotdog/00000011.jpg&#39;),Path(&#39;FastFood/Hotdog/00000002.jpg&#39;),Path(&#39;FastFood/Hotdog/00000008.jpg&#39;)...] . failed = verify_images(fns) failed . (#0) [] . failed.map(Path.unlink); . ls . 01_intro.html 09_tabular.html 17_foundations.html 01_intro.ipynb 09_tabular.ipynb 17_foundations.ipynb 02_production.html 10_nlp.html 18_CAM.html 02_production.ipynb 10_nlp.ipynb 18_CAM.ipynb 03_ethics.html 11_midlevel_data.html 19_learner.html 03_ethics.ipynb 11_midlevel_data.ipynb 19_learner.ipynb 04_mnist_basics.html 12_nlp_dive.html 20_conclusion.html 04_mnist_basics.ipynb 12_nlp_dive.ipynb 20_conclusion.ipynb 05_pet_breeds.html 13_convolutions.html FastFood/ 05_pet_breeds.ipynb 13_convolutions.ipynb app_blog.html 06_multicat.html 14_resnet.html app_blog.ipynb 06_multicat.ipynb 14_resnet.ipynb app_jupyter.html 07_sizing_and_tta.html 15_arch_details.html app_jupyter.ipynb 07_sizing_and_tta.ipynb 15_arch_details.ipynb export.pkl 08_collab.html 16_accel_sgd.html 08_collab.ipynb 16_accel_sgd.ipynb . Sidebar: Getting Help in Jupyter Notebooks . End sidebar . From Data to DataLoaders . hotdogs = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = hotdogs.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . hotdogs = hotdogs.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = hotdogs.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . hotdogs = hotdogs.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = hotdogs.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . hotdogs = hotdogs.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = hotdogs.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Data Augmentation . hotdogs = hotdogs.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = hotdogs.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Training Your Model, and Using It to Clean Your Data . hotdogs = hotdogs.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = hotdogs.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.182863 | 1.938827 | 0.466667 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.341504 | 0.629427 | 0.116667 | 00:02 | . 1 | 0.250607 | 0.448193 | 0.050000 | 00:02 | . 2 | 0.217635 | 0.409125 | 0.050000 | 00:02 | . 3 | 0.233157 | 0.359082 | 0.050000 | 00:02 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . cleaner = ImageClassifierCleaner(learn) cleaner . Turning Your Model into an Online Application . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . learn_inf.predict(&#39;FastFood/Hotdog/00000019.jpg&#39;) . (&#39;Hotdog&#39;, tensor(0), tensor([9.9976e-01, 2.4390e-04])) . learn_inf.dls.vocab . [&#39;Hotdog&#39;, &#39;Not Hotdog&#39;] . Creating a Notebook App from the Model . btn_upload = widgets.FileUpload() btn_upload . img = PILImage.create(btn_upload.data[-1]) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your hotdog!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Turning Your Notebook into a Real App . Deploying your app . How to Avoid Disaster . Unforeseen Consequences and Feedback Loops . Get Writing! . Questionnaire . Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain Approach? | How do the steps of the Drivetrain Approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are three examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out-of-domain data&quot;? | What is &quot;domain shift&quot;? | What are the three steps in the deployment process? | Further Research . Consider how the Drivetrain Approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;What would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. |",
            "url": "https://emil428f.github.io/Pba.Blog/2021/02/16/Hotdog-Deep-Learning.html",
            "relUrl": "/2021/02/16/Hotdog-Deep-Learning.html",
            "date": " • Feb 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://emil428f.github.io/Pba.Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://emil428f.github.io/Pba.Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://emil428f.github.io/Pba.Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}